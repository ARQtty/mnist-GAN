{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iria/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/iria/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/iria/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/iria/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/iria/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/iria/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    # Weight initializer\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "\n",
    "def plot(samples):\n",
    "    # 3x3 plot for show generative net's output\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/iria/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "mnist_shape = 784\n",
    "hidden_size = 256\n",
    "output_shape = 10\n",
    "\n",
    "input_D_ph = tf.placeholder(tf.float32, [None, mnist_shape], name='Real_X')\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init((mnist_shape, hidden_size)), name='D_W1')\n",
    "D_b1 = tf.Variable(tf.zeros(hidden_size), name='D_b1')\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init((hidden_size, output_shape)), name='D_W2')\n",
    "D_b2 = tf.Variable(tf.zeros(output_shape), name='D_b2')\n",
    "\n",
    "\n",
    "def discriminative_network(x):\n",
    "    layer1 = tf.matmul(x, D_W1) + D_b1\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    layer2 = tf.matmul(layer1, D_W2) + D_b2\n",
    "    layer2 = tf.nn.sigmoid(layer2)\n",
    "\n",
    "    return layer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 300\n",
    "input_G_ph = tf.placeholder(tf.float32, [None, z_dim], name='Fake_X')\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init((z_dim, hidden_size)), name='G_W1')\n",
    "G_b1 = tf.Variable(tf.zeros(hidden_size), name='G_b1')\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init((hidden_size, mnist_shape)), name='G_W2')\n",
    "G_b2 = tf.Variable(tf.zeros(mnist_shape), name='G_b2')\n",
    "\n",
    "\n",
    "def generative_network(z):\n",
    "    layer1 = tf.matmul(z, G_W1) + G_b1\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    \n",
    "    layer2 = tf.matmul(layer1, G_W2) + G_b2\n",
    "    layer2 = tf.nn.sigmoid(layer2)\n",
    "\n",
    "    return layer2\n",
    "\n",
    "\n",
    "def noise_generator(shape):\n",
    "    noise = np.random.uniform(-1., 1., size=shape)\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating net's outputs and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_input = generative_network(input_G_ph)\n",
    "\n",
    "D_real = discriminative_network(input_D_ph)\n",
    "D_fake = discriminative_network(fake_input)\n",
    "\n",
    "D_real_loss = tf.reduce_mean(tf.log(D_real))\n",
    "D_fake_loss = tf.reduce_mean(tf.log(1. - D_fake))\n",
    "D_loss = -(D_real_loss + D_fake_loss)\n",
    "\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "D_train = tf.train.AdamOptimizer(learning_rate=lr).minimize(D_loss, var_list=[D_W1, D_b1, D_W2, D_b2])\n",
    "G_train = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss, var_list=[G_W1, G_b1, G_W2, G_b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-c692fc537532>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/iria/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/iria/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/iria/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/iria/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/iria/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/iria/.local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 0   D loss: 1.02128   G loss: 1.53794\n",
      "Epoch 1   D loss: 0.79131   G loss: 1.96383\n",
      "Epoch 2   D loss: 0.61004   G loss: 2.20527\n",
      "Epoch 3   D loss: 0.46489   G loss: 2.38224\n",
      "Epoch 4   D loss: 0.36062   G loss: 2.56504\n",
      "Epoch 5   D loss: 0.28371   G loss: 2.74951\n",
      "Epoch 6   D loss: 0.23255   G loss: 2.89767\n",
      "Epoch 7   D loss: 0.19896   G loss: 3.00693\n",
      "Epoch 8   D loss: 0.17742   G loss: 3.08399\n",
      "Epoch 9   D loss: 0.16871   G loss: 3.15029\n",
      "Epoch 10   D loss: 0.16926   G loss: 3.16405\n",
      "Epoch 11   D loss: 0.17414   G loss: 3.16581\n",
      "Epoch 12   D loss: 0.19239   G loss: 3.11502\n",
      "Epoch 13   D loss: 0.21482   G loss: 3.05100\n",
      "Epoch 14   D loss: 0.24225   G loss: 2.96071\n",
      "Epoch 15   D loss: 0.28086   G loss: 2.80066\n",
      "Epoch 16   D loss: 0.31841   G loss: 2.67248\n",
      "Epoch 17   D loss: 0.36350   G loss: 2.52675\n",
      "Epoch 18   D loss: 0.41077   G loss: 2.40455\n",
      "Epoch 19   D loss: 0.46296   G loss: 2.26161\n",
      "Epoch 20   D loss: 0.52170   G loss: 2.12318\n",
      "Epoch 21   D loss: 0.56870   G loss: 2.02401\n",
      "Epoch 22   D loss: 0.60424   G loss: 1.93747\n",
      "Epoch 23   D loss: 0.63978   G loss: 1.87125\n",
      "Epoch 24   D loss: 0.65958   G loss: 1.83109\n",
      "Epoch 25   D loss: 0.66376   G loss: 1.82162\n",
      "Epoch 26   D loss: 0.65274   G loss: 1.82361\n",
      "Epoch 27   D loss: 0.64493   G loss: 1.82484\n",
      "Epoch 28   D loss: 0.62767   G loss: 1.83295\n",
      "Epoch 29   D loss: 0.61332   G loss: 1.87069\n",
      "Epoch 30   D loss: 0.60828   G loss: 1.87029\n",
      "Epoch 31   D loss: 0.59958   G loss: 1.87786\n",
      "Epoch 32   D loss: 0.59266   G loss: 1.87016\n",
      "Epoch 33   D loss: 0.58826   G loss: 1.87570\n",
      "Epoch 34   D loss: 0.58205   G loss: 1.87094\n",
      "Epoch 35   D loss: 0.56656   G loss: 1.90324\n",
      "Epoch 36   D loss: 0.53745   G loss: 1.94091\n",
      "Epoch 37   D loss: 0.51397   G loss: 1.97217\n",
      "Epoch 38   D loss: 0.49255   G loss: 2.01243\n",
      "Epoch 39   D loss: 0.47755   G loss: 2.02378\n",
      "Epoch 40   D loss: 0.46185   G loss: 2.04519\n",
      "Epoch 41   D loss: 0.44280   G loss: 2.06898\n",
      "Epoch 42   D loss: 0.42989   G loss: 2.09933\n",
      "Epoch 43   D loss: 0.41296   G loss: 2.11566\n",
      "Epoch 44   D loss: 0.40762   G loss: 2.11846\n",
      "Epoch 45   D loss: 0.39548   G loss: 2.13113\n",
      "Epoch 46   D loss: 0.38903   G loss: 2.14299\n",
      "Epoch 47   D loss: 0.37704   G loss: 2.15894\n",
      "Epoch 48   D loss: 0.37060   G loss: 2.19909\n",
      "Epoch 49   D loss: 0.35570   G loss: 2.22984\n",
      "Epoch 50   D loss: 0.34321   G loss: 2.26659\n",
      "Epoch 51   D loss: 0.32916   G loss: 2.31128\n",
      "Epoch 52   D loss: 0.31736   G loss: 2.33194\n",
      "Epoch 53   D loss: 0.30552   G loss: 2.34326\n",
      "Epoch 54   D loss: 0.30882   G loss: 2.36396\n",
      "Epoch 55   D loss: 0.29910   G loss: 2.37744\n",
      "Epoch 56   D loss: 0.29512   G loss: 2.37824\n",
      "Epoch 57   D loss: 0.28995   G loss: 2.38519\n",
      "Epoch 58   D loss: 0.27880   G loss: 2.41188\n",
      "Epoch 59   D loss: 0.28056   G loss: 2.41986\n",
      "Epoch 60   D loss: 0.27208   G loss: 2.41806\n",
      "Epoch 61   D loss: 0.27827   G loss: 2.41925\n",
      "Epoch 62   D loss: 0.27549   G loss: 2.40792\n",
      "Epoch 63   D loss: 0.28033   G loss: 2.40012\n",
      "Epoch 64   D loss: 0.28255   G loss: 2.36697\n",
      "Epoch 65   D loss: 0.28186   G loss: 2.35588\n",
      "Epoch 66   D loss: 0.27544   G loss: 2.35700\n",
      "Epoch 67   D loss: 0.27406   G loss: 2.35825\n",
      "Epoch 68   D loss: 0.26887   G loss: 2.36236\n",
      "Epoch 69   D loss: 0.26182   G loss: 2.38318\n",
      "Epoch 70   D loss: 0.26081   G loss: 2.37354\n",
      "Epoch 71   D loss: 0.26180   G loss: 2.38473\n",
      "Epoch 72   D loss: 0.26271   G loss: 2.36761\n",
      "Epoch 73   D loss: 0.25972   G loss: 2.37642\n",
      "Epoch 74   D loss: 0.25617   G loss: 2.38582\n",
      "Epoch 75   D loss: 0.25176   G loss: 2.36409\n",
      "Epoch 76   D loss: 0.24905   G loss: 2.36823\n",
      "Epoch 77   D loss: 0.24576   G loss: 2.35538\n",
      "Epoch 78   D loss: 0.25547   G loss: 2.33558\n",
      "Epoch 79   D loss: 0.25783   G loss: 2.27541\n",
      "Epoch 80   D loss: 0.27692   G loss: 2.23572\n",
      "Epoch 81   D loss: 0.27720   G loss: 2.22051\n",
      "Epoch 82   D loss: 0.28931   G loss: 2.18697\n",
      "Epoch 83   D loss: 0.29859   G loss: 2.15845\n",
      "Epoch 84   D loss: 0.31599   G loss: 2.12283\n",
      "Epoch 85   D loss: 0.32440   G loss: 2.09253\n",
      "Epoch 86   D loss: 0.33859   G loss: 2.04539\n",
      "Epoch 87   D loss: 0.34235   G loss: 2.02405\n",
      "Epoch 88   D loss: 0.33881   G loss: 2.02264\n",
      "Epoch 89   D loss: 0.33451   G loss: 2.02168\n",
      "Epoch 90   D loss: 0.32054   G loss: 2.03782\n",
      "Epoch 91   D loss: 0.31845   G loss: 2.02891\n",
      "Epoch 92   D loss: 0.32837   G loss: 2.01834\n",
      "Epoch 93   D loss: 0.31879   G loss: 2.01960\n",
      "Epoch 94   D loss: 0.31786   G loss: 1.99090\n",
      "Epoch 95   D loss: 0.33270   G loss: 1.97038\n",
      "Epoch 96   D loss: 0.34386   G loss: 1.93671\n",
      "Epoch 97   D loss: 0.34877   G loss: 1.90155\n",
      "Epoch 98   D loss: 0.36680   G loss: 1.85446\n",
      "Epoch 99   D loss: 0.37330   G loss: 1.81299\n",
      "Epoch 100   D loss: 0.39431   G loss: 1.77294\n",
      "Epoch 101   D loss: 0.40136   G loss: 1.74088\n",
      "Epoch 102   D loss: 0.41418   G loss: 1.71356\n",
      "Epoch 103   D loss: 0.43205   G loss: 1.66131\n",
      "Epoch 104   D loss: 0.43242   G loss: 1.63977\n",
      "Epoch 105   D loss: 0.43345   G loss: 1.64522\n",
      "Epoch 106   D loss: 0.43098   G loss: 1.61650\n",
      "Epoch 107   D loss: 0.43951   G loss: 1.61720\n",
      "Epoch 108   D loss: 0.42100   G loss: 1.63257\n",
      "Epoch 109   D loss: 0.41885   G loss: 1.64058\n",
      "Epoch 110   D loss: 0.42989   G loss: 1.61807\n",
      "Epoch 111   D loss: 0.44821   G loss: 1.58337\n",
      "Epoch 112   D loss: 0.47546   G loss: 1.54932\n",
      "Epoch 113   D loss: 0.50314   G loss: 1.49557\n",
      "Epoch 114   D loss: 0.50999   G loss: 1.49515\n",
      "Epoch 115   D loss: 0.51255   G loss: 1.50438\n",
      "Epoch 116   D loss: 0.50218   G loss: 1.49847\n",
      "Epoch 117   D loss: 0.50760   G loss: 1.48910\n",
      "Epoch 118   D loss: 0.51043   G loss: 1.46433\n",
      "Epoch 119   D loss: 0.52611   G loss: 1.43944\n",
      "Epoch 120   D loss: 0.54229   G loss: 1.38851\n",
      "Epoch 121   D loss: 0.56440   G loss: 1.36902\n",
      "Epoch 122   D loss: 0.57709   G loss: 1.35634\n",
      "Epoch 123   D loss: 0.55697   G loss: 1.35779\n",
      "Epoch 124   D loss: 0.57738   G loss: 1.36613\n",
      "Epoch 125   D loss: 0.55802   G loss: 1.39084\n",
      "Epoch 126   D loss: 0.56902   G loss: 1.38514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127   D loss: 0.58814   G loss: 1.35965\n",
      "Epoch 128   D loss: 0.60085   G loss: 1.35409\n",
      "Epoch 129   D loss: 0.60861   G loss: 1.32297\n",
      "Epoch 130   D loss: 0.60178   G loss: 1.34032\n",
      "Epoch 131   D loss: 0.58839   G loss: 1.35597\n",
      "Epoch 132   D loss: 0.57371   G loss: 1.36265\n",
      "Epoch 133   D loss: 0.57226   G loss: 1.36428\n",
      "Epoch 134   D loss: 0.56793   G loss: 1.37715\n",
      "Epoch 135   D loss: 0.56498   G loss: 1.38587\n",
      "Epoch 136   D loss: 0.53931   G loss: 1.39953\n",
      "Epoch 137   D loss: 0.53037   G loss: 1.42134\n",
      "Epoch 138   D loss: 0.52380   G loss: 1.42655\n",
      "Epoch 139   D loss: 0.51945   G loss: 1.41925\n",
      "Epoch 140   D loss: 0.52661   G loss: 1.40658\n",
      "Epoch 141   D loss: 0.51163   G loss: 1.43687\n",
      "Epoch 142   D loss: 0.50241   G loss: 1.44359\n",
      "Epoch 143   D loss: 0.49400   G loss: 1.45621\n",
      "Epoch 144   D loss: 0.47534   G loss: 1.48568\n",
      "Epoch 145   D loss: 0.47462   G loss: 1.51211\n",
      "Epoch 146   D loss: 0.44245   G loss: 1.55463\n",
      "Epoch 147   D loss: 0.45582   G loss: 1.55467\n",
      "Epoch 148   D loss: 0.45094   G loss: 1.54356\n",
      "Epoch 149   D loss: 0.47062   G loss: 1.53570\n",
      "Epoch 150   D loss: 0.48600   G loss: 1.51278\n",
      "Epoch 151   D loss: 0.50567   G loss: 1.49675\n",
      "Epoch 152   D loss: 0.52085   G loss: 1.45930\n",
      "Epoch 153   D loss: 0.52465   G loss: 1.44837\n",
      "Epoch 154   D loss: 0.52886   G loss: 1.44148\n",
      "Epoch 155   D loss: 0.51089   G loss: 1.48415\n",
      "Epoch 156   D loss: 0.47889   G loss: 1.53206\n",
      "Epoch 157   D loss: 0.45694   G loss: 1.57007\n",
      "Epoch 158   D loss: 0.43645   G loss: 1.61315\n",
      "Epoch 159   D loss: 0.42717   G loss: 1.62718\n",
      "Epoch 160   D loss: 0.41564   G loss: 1.63277\n",
      "Epoch 161   D loss: 0.42376   G loss: 1.62296\n",
      "Epoch 162   D loss: 0.42120   G loss: 1.64421\n",
      "Epoch 163   D loss: 0.40334   G loss: 1.66047\n",
      "Epoch 164   D loss: 0.39127   G loss: 1.72321\n",
      "Epoch 165   D loss: 0.38242   G loss: 1.72732\n",
      "Epoch 166   D loss: 0.37579   G loss: 1.74992\n",
      "Epoch 167   D loss: 0.37656   G loss: 1.74110\n",
      "Epoch 168   D loss: 0.37256   G loss: 1.74587\n",
      "Epoch 169   D loss: 0.36391   G loss: 1.77617\n",
      "Epoch 170   D loss: 0.35103   G loss: 1.79937\n",
      "Epoch 171   D loss: 0.33454   G loss: 1.82042\n",
      "Epoch 172   D loss: 0.33831   G loss: 1.82225\n",
      "Epoch 173   D loss: 0.33778   G loss: 1.81378\n",
      "Epoch 174   D loss: 0.34068   G loss: 1.81516\n",
      "Epoch 175   D loss: 0.35019   G loss: 1.82464\n",
      "Epoch 176   D loss: 0.34613   G loss: 1.83589\n",
      "Epoch 177   D loss: 0.33657   G loss: 1.87475\n",
      "Epoch 178   D loss: 0.32044   G loss: 1.92579\n",
      "Epoch 179   D loss: 0.31802   G loss: 1.93392\n",
      "Epoch 180   D loss: 0.30258   G loss: 1.96482\n",
      "Epoch 181   D loss: 0.31318   G loss: 1.92379\n",
      "Epoch 182   D loss: 0.32034   G loss: 1.91402\n",
      "Epoch 183   D loss: 0.32996   G loss: 1.91532\n",
      "Epoch 184   D loss: 0.33113   G loss: 1.92446\n",
      "Epoch 185   D loss: 0.32864   G loss: 1.92794\n",
      "Epoch 186   D loss: 0.34141   G loss: 1.89977\n",
      "Epoch 187   D loss: 0.34729   G loss: 1.91036\n",
      "Epoch 188   D loss: 0.34977   G loss: 1.90179\n",
      "Epoch 189   D loss: 0.34953   G loss: 1.91218\n",
      "Epoch 190   D loss: 0.34990   G loss: 1.88258\n",
      "Epoch 191   D loss: 0.35623   G loss: 1.86625\n",
      "Epoch 192   D loss: 0.35064   G loss: 1.88514\n",
      "Epoch 193   D loss: 0.34344   G loss: 1.89781\n",
      "Epoch 194   D loss: 0.34787   G loss: 1.91409\n",
      "Epoch 195   D loss: 0.34515   G loss: 1.92387\n",
      "Epoch 196   D loss: 0.32924   G loss: 1.96605\n",
      "Epoch 197   D loss: 0.31860   G loss: 1.97078\n",
      "Epoch 198   D loss: 0.31939   G loss: 2.02128\n",
      "Epoch 199   D loss: 0.31926   G loss: 2.01179\n",
      "Epoch 200   D loss: 0.32946   G loss: 1.98724\n",
      "Epoch 201   D loss: 0.34149   G loss: 1.95385\n",
      "Epoch 202   D loss: 0.35186   G loss: 1.93141\n",
      "Epoch 203   D loss: 0.36502   G loss: 1.86118\n",
      "Epoch 204   D loss: 0.38101   G loss: 1.82699\n",
      "Epoch 205   D loss: 0.40751   G loss: 1.78212\n",
      "Epoch 206   D loss: 0.40769   G loss: 1.79057\n",
      "Epoch 207   D loss: 0.40113   G loss: 1.81128\n",
      "Epoch 208   D loss: 0.39141   G loss: 1.83168\n",
      "Epoch 209   D loss: 0.39597   G loss: 1.82622\n",
      "Epoch 210   D loss: 0.39172   G loss: 1.81368\n",
      "Epoch 211   D loss: 0.40917   G loss: 1.78384\n",
      "Epoch 212   D loss: 0.42871   G loss: 1.73604\n",
      "Epoch 213   D loss: 0.42299   G loss: 1.74484\n",
      "Epoch 214   D loss: 0.43098   G loss: 1.71511\n",
      "Epoch 215   D loss: 0.44499   G loss: 1.68184\n",
      "Epoch 216   D loss: 0.45201   G loss: 1.67329\n",
      "Epoch 217   D loss: 0.45530   G loss: 1.67172\n",
      "Epoch 218   D loss: 0.44740   G loss: 1.71325\n",
      "Epoch 219   D loss: 0.44066   G loss: 1.70935\n",
      "Epoch 220   D loss: 0.44692   G loss: 1.71685\n",
      "Epoch 221   D loss: 0.44520   G loss: 1.72456\n",
      "Epoch 222   D loss: 0.42485   G loss: 1.75888\n",
      "Epoch 223   D loss: 0.41802   G loss: 1.75176\n",
      "Epoch 224   D loss: 0.41754   G loss: 1.78638\n",
      "Epoch 225   D loss: 0.40815   G loss: 1.82125\n",
      "Epoch 226   D loss: 0.40515   G loss: 1.83280\n",
      "Epoch 227   D loss: 0.42446   G loss: 1.83732\n",
      "Epoch 228   D loss: 0.44307   G loss: 1.83997\n",
      "Epoch 229   D loss: 0.42951   G loss: 1.83301\n"
     ]
    }
   ],
   "source": [
    "mnist_ex = input_data.read_data_sets(\"./data/\", one_hot=True)\n",
    "\n",
    "epoches = 1500\n",
    "batch_size = 4096\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement=True\n",
    "sess = tf.Session()#config=config)\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "with sess:\n",
    "    for e in range(epoches):\n",
    "        d_losses, g_losses = [], []\n",
    "        \n",
    "        if e % 5 == 0:\n",
    "            noise = noise_generator((9, z_dim))\n",
    "            samples = sess.run(generative_network(input_G_ph), feed_dict={input_G_ph: noise})\n",
    "\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('./out/{}.png'.format(str(e).zfill(3)), bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "            \n",
    "        \n",
    "        for step in range(mnist_ex.train.num_examples // batch_size):\n",
    "            batchX, _ = mnist_ex.train.next_batch(batch_size)\n",
    "            noise = noise_generator((batch_size, z_dim))\n",
    "            \n",
    "            d_loss, _ = sess.run([D_loss, D_train], feed_dict={input_D_ph: batchX, input_G_ph: noise})\n",
    "            g_loss, _ = sess.run([G_loss, G_train], feed_dict={input_G_ph: noise})\n",
    "            \n",
    "            d_losses.append(d_loss)\n",
    "            g_losses.append(g_loss)\n",
    "        \n",
    "        print(\"Epoch %d   D loss: %.5f   G loss: %.5f\" % (e, d_loss, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
